name: Compile MLC-LLM for iOS

on:
  workflow_dispatch:

jobs:
  compile-ios:
    runs-on: macos-14
    timeout-minutes: 120
    
    steps:
    - name: Setup Conda
      uses: conda-incubator/setup-miniconda@v3
      with:
        auto-update-conda: true
        python-version: '3.11'
        activate-environment: mlc
        
    - name: Install MLC-LLM from pip (pre-built)
      shell: bash -el {0}
      run: |
        # Try installing pre-built mlc-llm
        pip install --pre mlc-llm-nightly mlc-ai-nightly -f https://mlc.ai/wheels || true
        pip install "huggingface_hub[cli]" safetensors transformers torch tqdm sentencepiece tiktoken
        
        # Check if mlc_llm works
        python -c "import mlc_llm; print('mlc_llm OK')" || echo "Need to build from source"
        
    - name: Build from source if needed
      shell: bash -el {0}
      run: |
        # Check if already available
        if python -c "import mlc_llm" 2>/dev/null; then
          echo "MLC-LLM already available"
          exit 0
        fi
        
        echo "Building from source..."
        pip install numpy decorator attrs typing_extensions psutil scipy tornado
        
        git clone --recursive https://github.com/mlc-ai/mlc-llm.git
        cd mlc-llm
        
        # Use cmake preset for simpler build
        pip install cmake ninja
        
        cd python
        pip install -e . --no-build-isolation || echo "Trying alternative install"
        cd ..
        
        # If that didn't work, try manual build
        if ! python -c "import mlc_llm" 2>/dev/null; then
          mkdir -p build && cd build
          
          cat > config.cmake << 'EOF'
        set(USE_METAL ON)
        set(USE_CUDA OFF)
        set(USE_VULKAN OFF)
        set(USE_OPENCL OFF)
        set(USE_FLASHINFER OFF)
        set(CMAKE_BUILD_TYPE Release)
        EOF
          
          cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_POLICY_VERSION_MINIMUM=3.5
          make -j$(sysctl -n hw.ncpu)
          cd ..
          
          # Set paths
          echo "export TVM_LIBRARY_PATH=$(pwd)/build/tvm" >> $GITHUB_ENV
          echo "export PYTHONPATH=$(pwd)/python:$(pwd)/3rdparty/tvm/python:\$PYTHONPATH" >> $GITHUB_ENV
        fi
        
    - name: Download Qwen3-4B Model
      shell: bash -el {0}
      run: |
        python << 'PYEOF'
        from huggingface_hub import snapshot_download
        snapshot_download(repo_id='Qwen/Qwen3-4B', local_dir='./qwen3-4b-base')
        print("Download complete")
        PYEOF
        
    - name: Generate Config and Compile
      shell: bash -el {0}
      run: |
        # Set paths if built from source
        if [ -d "mlc-llm" ]; then
          cd mlc-llm
          export TVM_LIBRARY_PATH=$(pwd)/build/tvm
          export DYLD_LIBRARY_PATH=$(pwd)/build/tvm:$DYLD_LIBRARY_PATH
          export PYTHONPATH=$(pwd)/python:$(pwd)/3rdparty/tvm/python:$PYTHONPATH
          cd ..
        fi
        
        mkdir -p output
        
        mlc_llm gen_config ./qwen3-4b-base \
          --quantization q4f16_1 \
          --prefill-chunk-size 128 \
          --context-window-size 4096 \
          --max-batch-size 1 \
          --conv-template qwen2 \
          --output ./output/config
        
        cat ./output/config/mlc-chat-config.json
        
        mlc_llm convert_weight ./qwen3-4b-base \
          --quantization q4f16_1 \
          --output ./output/weights
        
        mlc_llm compile ./output/config/mlc-chat-config.json \
          --device iphone \
          --output ./output/model-iphone.tar
        
    - name: Extract compiled files
      run: |
        cd output
        tar -xvf model-iphone.tar
        ls -lh *.o 2>/dev/null || echo "No .o files"
        grep -E "prefill|context_window" config/mlc-chat-config.json || true
        
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: ios-compiled-libs
        path: |
          output/*.o
          output/config/mlc-chat-config.json
        retention-days: 30
