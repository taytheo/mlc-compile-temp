name: Compile MLC-LLM for iOS

on:
  workflow_dispatch:

jobs:
  compile-ios:
    runs-on: macos-14
    timeout-minutes: 180

    defaults:
      run:
        shell: bash -el {0}

    steps:
    - name: Setup Conda
      uses: conda-incubator/setup-miniconda@v3
      with:
        auto-update-conda: true
        python-version: "3.11"
        activate-environment: mlc

    - name: Install MLC-LLM Python package
      run: |
        pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cpu mlc-ai-nightly-cpu
        pip install huggingface_hub

    - name: Patch batch_spec_verify.py
      run: |
        SITE_PKG=$(python -c "import site; print(site.getsitepackages()[0])")
        BSV="$SITE_PKG/mlc_llm/op/batch_spec_verify.py"
        echo "=== BEFORE PATCH ==="
        grep -n 'alloc_buffer.*bool\|done\[0\]\|while.*T\.Not' "$BSV" | head -20 || echo "Pattern not found"
        
        # sed 명령어로 패치 적용
        sed -i '' 's/T\.alloc_buffer((1,), "bool")/T.alloc_buffer((1,), "int32")/g' "$BSV"
        sed -i '' 's/done\[0\] = False/done[0] = 0/g' "$BSV"
        sed -i '' 's/done\[0\] = True/done[0] = 1/g' "$BSV"
        sed -i '' 's/while T\.Not(done\[0\]):/while done[0] == 0:/g' "$BSV"
        
        echo "Patch applied to batch_spec_verify.py"
        
        echo "=== AFTER PATCH ==="
        grep -n 'alloc_buffer.*int32\|done\[0\] ==\|done\[0\] = 0\|done\[0\] = 1' "$BSV" | head -20 || echo "Verify failed"

    - name: Download model and create custom config
      run: |
        mkdir -p model_weights
        python -c "from huggingface_hub import snapshot_download; snapshot_download(repo_id='mlc-ai/Qwen3-4B-q4f16_1-MLC', local_dir='./model_weights/Qwen3-4B-q4f16_1-MLC')"
        cp ./model_weights/Qwen3-4B-q4f16_1-MLC/mlc-chat-config.json ./model_weights/Qwen3-4B-q4f16_1-MLC/mlc-chat-config.json.bak
        
        # Python 스크립트 파일 생성
        echo 'import json
with open("./model_weights/Qwen3-4B-q4f16_1-MLC/mlc-chat-config.json", "r") as f:
    config = json.load(f)
config["prefill_chunk_size"] = 128
config["context_window_size"] = 4096
config["model_config"]["prefill_chunk_size"] = 128
config["model_config"]["context_window_size"] = 4096
config["model_config"]["max_batch_size"] = 1
config["model_config"]["dtype"] = "float16"
with open("./model_weights/Qwen3-4B-q4f16_1-MLC/mlc-chat-config.json", "w") as f:
    json.dump(config, f, indent=2)
print("Config updated")
print("  prefill_chunk_size: " + str(config["prefill_chunk_size"]))
print("  context_window_size: " + str(config["context_window_size"]))
print("  max_batch_size: " + str(config["model_config"]["max_batch_size"]))' > config_script.py
        
        python3 config_script.py
        rm config_script.py

    - name: Compile for iOS
      run: |
        mkdir -p ./output
        echo "=== Starting compilation ==="
        python -m mlc_llm compile \
          ./model_weights/Qwen3-4B-q4f16_1-MLC/mlc-chat-config.json \
          --device iphone \
          --output ./output/qwen3_q4f16_1-iphone.tar \
          2>&1 | tee compile_log.txt
        echo "=== Compilation output ==="
        ls -la ./output/

    - name: Extract compiled files
      if: success()
      run: |
        cd output
        if [ -f qwen3_q4f16_1-iphone.tar ]; then
          tar -xvf qwen3_q4f16_1-iphone.tar
          echo "=== Extracted files ==="
          ls -lh
        fi

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: qwen3-4b-ios-compiled
        path: |
          output/*.o
          output/*.tar
          model_weights/Qwen3-4B-q4f16_1-MLC/mlc-chat-config.json
          compile_log.txt
        retention-days: 30
        if-no-files-found: warn
