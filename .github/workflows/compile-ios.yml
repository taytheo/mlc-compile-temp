name: Compile MLC-LLM for iOS

on:
  workflow_dispatch:

jobs:
  compile-ios:
    runs-on: macos-14
    timeout-minutes: 180

    defaults:
      run:
        shell: bash -el {0}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Conda
        uses: conda-incubator/setup-miniconda@v3
        with:
          auto-update-conda: true
          python-version: "3.11"
          activate-environment: mlc

      - name: Install MLC-LLM Python package
        run: |
          pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cpu mlc-ai-nightly-cpu
          pip install huggingface_hub

      - name: Patch MLC-LLM bool type bug
        run: |
          echo "üîß MLC-LLM Bool ÌÉÄÏûÖ Î≤ÑÍ∑∏ Ìå®Ïπò ÏãúÏûë (GitHub Issue #3389)"
          python3 .github/scripts/patch_mlc_bool_bug.py

      - name: Download model weights
        run: |
          mkdir -p model_weights
          echo "üì• Downloading Qwen3-4B model weights..."
          python -c "from huggingface_hub import snapshot_download; snapshot_download(repo_id='mlc-ai/Qwen3-4B-q4f16_1-MLC', local_dir='./model_weights/Qwen3-4B-q4f16_1-MLC')"

      - name: Modify config for iPhone low memory
        run: |
          echo "‚öôÔ∏è Modifying mlc-chat-config.json for iPhone (low memory)..."
          cd ./model_weights/Qwen3-4B-q4f16_1-MLC
          # ÏõêÎ≥∏ Î∞±ÏóÖ
          cp mlc-chat-config.json mlc-chat-config.json.bak
          # PythonÏúºÎ°ú config ÏàòÏ†ï
          python3 << 'EOF'
          import json
          
          with open('mlc-chat-config.json', 'r') as f:
              config = json.load(f)
          
          # Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
          config['prefill_chunk_size'] = 32
          config['context_window_size'] = 1024
          
          if 'model_config' in config:
              config['model_config']['prefill_chunk_size'] = 32
              config['model_config']['context_window_size'] = 1024
              config['model_config']['max_batch_size'] = 1
          
          with open('mlc-chat-config.json', 'w') as f:
              json.dump(config, f, indent=2)
          
          print("‚úÖ Config updated:")
          print(f"   prefill_chunk_size: {config['prefill_chunk_size']}")
          print(f"   context_window_size: {config['context_window_size']}")
          EOF
          cat mlc-chat-config.json

      - name: Compile for iOS
        run: |
          mkdir -p ./output
          echo "üî® Starting iOS compilation with optimized config..."
          python -m mlc_llm compile \
            ./model_weights/Qwen3-4B-q4f16_1-MLC/mlc-chat-config.json \
            --device iphone \
            --output ./output/qwen3_q4f16_1-iphone.tar \
            2>&1 | tee compile_log.txt
          echo "=== Compilation output ==="
          ls -la ./output/

      - name: Extract compiled files
        if: success()
        run: |
          cd output
          if [ -f qwen3_q4f16_1-iphone.tar ]; then
            tar -xvf qwen3_q4f16_1-iphone.tar
            echo "=== Extracted files ==="
            ls -lh
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: qwen3-4b-ios-compiled-lowmem
          path: |
            output/*.o
            output/*.tar
            model_weights/Qwen3-4B-q4f16_1-MLC/mlc-chat-config.json
            model_weights/Qwen3-4B-q4f16_1-MLC/mlc-chat-config.json.bak
            compile_log.txt
          retention-days: 30
          if-no-files-found: warn
