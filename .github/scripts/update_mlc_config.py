#!/usr/bin/env python3
"""
MLC ëª¨ë¸ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ìŠ¤í¬ë¦½íŠ¸
iOS ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ ë²„í¼ ì„¤ì • ì ìš©
"""

import json
import sys

CONFIG_PATH = './model_weights/Qwen3-4B-q4f16_1-MLC/mlc-chat-config.json'

def main():
    print(f"ğŸ“„ ì„¤ì • íŒŒì¼ ë¡œë“œ: {CONFIG_PATH}")
    
    with open(CONFIG_PATH, 'r') as f:
        config = json.load(f)
    
    # iOS ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • (iPhone 6GB GPU ë©”ëª¨ë¦¬ íƒ€ê²Ÿ)
    # ê¸°ì¡´: prefill_chunk_size=128, context=4096 â†’ ë²„í¼ 2.8GB (ë„ˆë¬´ í¼)
    # ë³€ê²½: prefill_chunk_size=32, context=1024 â†’ ë²„í¼ ~700MB (ì í•©)
    config['prefill_chunk_size'] = 32
    config['context_window_size'] = 1024
    
    # ëª¨ë¸ ì„¤ì •
    if 'model_config' not in config:
        config['model_config'] = {}
    
    config['model_config']['prefill_chunk_size'] = 32
    config['model_config']['context_window_size'] = 1024
    config['model_config']['max_batch_size'] = 1
    config['model_config']['dtype'] = 'float16'
    
    with open(CONFIG_PATH, 'w') as f:
        json.dump(config, f, indent=2)
    
    print("âœ… ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ:")
    print(f"   prefill_chunk_size: {config['prefill_chunk_size']}")
    print(f"   context_window_size: {config['context_window_size']}")
    print(f"   max_batch_size: {config['model_config']['max_batch_size']}")
    print(f"   dtype: {config['model_config']['dtype']}")

if __name__ == '__main__':
    main()
